[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Hi! My name is Admir Cesar de Oliveira Junior. I’m a mix of a biologist and a mathematician, but above all, I’m an absolute nerd. I love learning, developing, researching, and teaching.\nMy academic work focuses on theoretical and computational ecology, especially using machine learning. I also have experience as a math and biology teacher and as a data analyst/scientist. This has helped me develop expertise in programming and data analysis. Essentially, I’m able to transform large, messy datasets into valuable insights using statistical and machine learning models—and a bit of magic. I’m equally capable at extracting meaningful information with little data. I can also create beautiful visualizations to express data. I’m proficient in R, Python, C++ and SQL, and I’m currently learning Julia. Additionally, I have advanced knowledge of Excel and intermediate skills in power BI.\nThroughout my experiences, I’ve developed a wide range of hard and soft skills. One of the most important, to me, is the ability to communicate data and findings effectively to people of all ages and backgrounds. I’ve also worked on several projects involving machine learning, AI, and statistical data analysis, where I’ve learned how to collaborate with background diverse teams and find ways to make it work.\nIn addition to all of this, I’m a big fan of history, geography, and literature. I can play many instruments (Brazilian viola is my favorite) and could talk about the Roman Empire for hours. I’m also a huge fan of Dune, The Lord of the Rings, and romantic poetry. On the left is a photo of me, happy to be meeting my nearly-namesake, Julius Caesar, in Uruguay."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Admir C. de O. Junior",
    "section": "",
    "text": "Hello, nice to meet you!\nI’m a biologist and mathematician with skills in statistics, data science and programming.\nAcademically, I’m mostly interested in all aspects of ecological modeling—from theory to practice, and its implications.\nThis website serves as my portfolio. Here, you will find information about me, my work, and side projects.\nFeel free to reach out!"
  },
  {
    "objectID": "projects/pythagoras_grouping/index.html",
    "href": "projects/pythagoras_grouping/index.html",
    "title": "Grouping students with the help of Pythagoras",
    "section": "",
    "text": "df &lt;- data.frame(\n  student = c(\"vitor\", \"luis\", \"arthur\", \"murilo\"),\n  music  =  c(\"melodic metal\", \"vaneira\", \"rock\", \"pagode\"),\n  sports =  c(\"tennis\", \"football\", \"table tennis\", \"football\"),\n  food   =  c(\"soap\", \"churrasco\", \"beans\", \"strogonoff\")\n)\n\ndf\n\n  student         music       sports       food\n1   vitor melodic metal       tennis       soap\n2    luis       vaneira     football  churrasco\n3  arthur          rock table tennis      beans\n4  murilo        pagode     football strogonoff\n\n\nOne thing I always tell my students is that nearly everything in the world can be quantified — as long as we’re okay with the consequences of doing so. For example, let’s now transform these categorical variables into numeric ones. To do this, we need to establish a rule. Since we want to measure how different my students’ preferences are, I’ll arbitrarily define ordered sets for each category, placing the most contrasting items at opposite ends. For music, I’d say the set is: {melodic metal, rock, pagode, vaneira}. This could be interpreted as a kind of ‘Brazilian-countryness’ scale, for instance — but the key idea is that ‘melodic metal’ is more similar (and thus closer on the scale) to ‘rock’ than to ‘vaneira’. For sports, the set is: {football, tennis, table tennis}. For food, the set is: {soap, beans, strogonoff, churrasco}. Guessing the scale I had in mind for the last two sets is left as an exercise for the reader (always wanted to say that). Food is the trickiest one — but I think we can all agree that soup is very different from roast beef… or at least I hope so. Also, Vitor eating soap is a inside joke of them. Don’t ask me. I’m afraid to know. Sets defined and ordered, let’s replace the categorical values for numerical ones, in our case, it will be simply the position in the set, what is highly questionable. To do this, I will use dplyr::case_when, what is an overkill, but I want to showoff my skills.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndf &lt;- df %&gt;% \n  mutate(\n    music = case_match(\n      music,\n      \"melodic metal\" ~ 1,\n      \"rock\" ~ 2,\n      \"pagode\" ~ 3,\n      \"vaneira\" ~ 4\n    ),\n    sports = case_match(\n      sports,\n      \"football\" ~ 1,\n      \"tennis\" ~ 2,\n      \"table tennis\" ~ 3\n    ),\n    food = case_match(\n      food,\n      \"soap\" ~ 1,\n      \"beans\" ~ 2,\n      \"strogonoff\" ~ 3,\n      \"churrasco\" ~ 4\n    )\n  )\n\nNice! Now we begin our task. Firstly, the one-dimensional distance between each student for each subject is simply the numerical difference; i.e, \\(d = |x_2 - x_1|\\), being \\(d\\) the distance, and \\(x_1\\) and \\(x_2\\) two points (students) in that dimension. Now, for two-dimensional distances, things start to get interesting. Let’s plot the points with music and food as axis:\n\nlibrary(ggplot2)\n\nggplot(data = df) +\n  geom_point(aes(x = music, y = food)) +\n  geom_text(aes(x = music, y = food, label = student), \n            hjust = -0.2, vjust = -0.2, check_overlap = TRUE)\n\n\n\n\n\n\n\n\nWell, pretty organized. I like it. In our task, the knowledge popularized by a fellow mathematician more than 2,500 years ago will come in very handy. Of course, I’m talking about Pythagoras and the Pythagorean theorem. The idea is that I can draw right triangles using each pair of points (students). For example, I can choose Vitor and Arthur, or Vitor and Luis:\n\nggplot(data = df) +\n  geom_point(aes(x = music, y = food)) +\n  geom_text(aes(x = music, y = food, label = student), \n            hjust = -0.2, vjust = -0.2, check_overlap = TRUE) +\n  annotate(\"segment\", x = 1, xend = 4, y = 1, yend = 4, color = \"purple\") +\n  annotate(\"segment\", x = 1, xend = 4, y = 1, yend = 1, color = \"green\") +\n  annotate(\"segment\", x = 4, xend = 4, y = 1, yend = 4, color = \"green\")+\n  annotate(\"segment\", x = 1, xend = 2, y = 1, yend = 2, color = \"blue\") +\n  annotate(\"segment\", x = 1, xend = 2, y = 1, yend = 1, color = \"red\") +\n  annotate(\"segment\", x = 2, xend = 2, y = 1, yend = 2, color = \"red\")\n\n\n\n\n\n\n\n\nAnd you see now that the distance between each point is the hypotenuse of that right triangle, and the catheti are the one-dimensional distances we mentioned earlier. That’s awesome, because humanity has known for at least 4,000 years now that the sum of the squares of the catheti equals the square of the hypotenuse; i.e., \\(h²=a²+b²\\), being \\(h\\) the hypotenuse and \\(a\\) and \\(b\\) the catheti. Since the hypotenuse represents the distance—known as the Euclidean distance, named after Euclid, the father of geometry—we can work with it this way:\n\\[\nh²=a²+b² \\rightarrow d²=(x_2-x_1)²+(y_2-y_1)² \\rightarrow d=\\sqrt{(x_2-x_1)²+(y_2-y_1)²}\n\\]\nSo, knowing that, we can create a function to calculate it for us:\n\ndistance &lt;- function(x1,y1,x2,y2) {\n  a &lt;- x2-x1\n  b &lt;- y2-y1\n  \n  d = sqrt((a^2)+(b^2))\n  \n  return(d)\n}\n\nTo make sure that it’s working, let’s test it with the classical example: if one cathetus equals 3 and the other equals 4, the hypotenuse should equal 5:\n\ndistance(0,0,3,4)\n\n[1] 5\n\n\nThere we go. Now we can calculate the distance between each possible pair of students. This type of information is better represented by a matrix; in our case, a 4x4 matrix:\n\ndistance_matrix &lt;- matrix(nrow=4,ncol=4)\nrow.names(distance_matrix) &lt;- colnames(distance_matrix) &lt;- unique(df$student)\ndistance_matrix\n\n       vitor luis arthur murilo\nvitor     NA   NA     NA     NA\nluis      NA   NA     NA     NA\narthur    NA   NA     NA     NA\nmurilo    NA   NA     NA     NA\n\n\nNow, let’s calculate the distances:\n\nfor (name_r in rownames(distance_matrix)) {\n  for (name_c in colnames(distance_matrix)) {\n    x1 &lt;- df[df$student==name_r,\"music\"]\n    y1 &lt;- df[df$student==name_r,\"food\"]\n    \n    x2 &lt;- df[df$student==name_c,\"music\"]\n    y2 &lt;- df[df$student==name_c,\"food\"]\n    \n    distance_matrix[name_r,name_c] &lt;- distance(x1,y1,x2,y2)\n  }\n}\n\ndistance_matrix\n\n          vitor     luis   arthur   murilo\nvitor  0.000000 4.242641 1.414214 2.828427\nluis   4.242641 0.000000 2.828427 1.414214\narthur 1.414214 2.828427 0.000000 1.414214\nmurilo 2.828427 1.414214 1.414214 0.000000\n\n\nPretty cool. Now we want to vizualise the groups formed. To do this, we need to clean the matrix a bit, formatting it to a distance matrix:\n\ndistance_matrix &lt;- as.dist(distance_matrix)\ndistance_matrix\n\n          vitor     luis   arthur\nluis   4.242641                  \narthur 1.414214 2.828427         \nmurilo 2.828427 1.414214 1.414214\n\n\nBetter. Now let’s perform a hierarchical clustering, which means we’ll see which students are closer to each other and create groups based on that.\nThis process is way too complex to implement from scratch here, like we did with the Euclidean distance, so we’ll use the classic stats::hclust() function instead:\n\nhc &lt;- hclust(distance_matrix)\nplot(hc)\n\n\n\n\n\n\n\n\nPreeety cool. But two-dimensional things are kinda lame. The real deal is n-dimensional.\nHowever, for the sake of human-possible visualizations, we’ll expand our idea to just three dimensions, using all subjects.\nLet’s visualize the points in a 3D plot:\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nplot_ly(df, x = ~music, y = ~food, z = ~sports,\n        type = 'scatter3d', mode = 'text+markers',\n        text = ~student, marker = list(size = 5))\n\n\n\n\n\nThe group structure seems to be the same, even in 3D. To test this, we would need to calculate the euclidean distance on this three-dimensional space for each pair of points. Turns out that the pythagorean theorem is so neat that it works with any number of dimensions. The reader can easily verify this with basic geometry, and will find the equation \\[\nd=\\sqrt{(x_2-x_1)²+(y_2-y_1)²+(z_2-z_1)²}\n\\] in which \\(d\\) is the euclidean distance, \\(x\\), \\(y\\) and \\(z\\) are the values for the two points in each dimension. Now, let’s modify our function to adapt it to three dimensions:\n\ndistance &lt;- function(x1,y1,x2,y2,z1,z2) {\n  a &lt;- x2-x1\n  b &lt;- y2-y1\n  c &lt;- z2-z1\n  \n  d = sqrt((a^2)+(b^2)+(c^2))\n  \n  return(d)\n}\n\nAnd calculate again the matrix:\n\ndistance_matrix &lt;- matrix(nrow=4,ncol=4)\nrow.names(distance_matrix) &lt;- colnames(distance_matrix) &lt;- unique(df$student)\n\nfor (name_r in rownames(distance_matrix)) {\n  for (name_c in colnames(distance_matrix)) {\n    x1 &lt;- df[df$student==name_r,\"music\"]\n    y1 &lt;- df[df$student==name_r,\"food\"]\n    z1 &lt;- df[df$student==name_r,\"sports\"]\n    \n    x2 &lt;- df[df$student==name_c,\"music\"]\n    y2 &lt;- df[df$student==name_c,\"food\"]\n    z2 &lt;- df[df$student==name_c,\"sports\"]\n    \n    distance_matrix[name_r,name_c] &lt;- distance(x1,y1,x2,y2,z1,z2)\n  }\n}\n\ndistance_matrix\n\n          vitor     luis   arthur   murilo\nvitor  0.000000 4.358899 1.732051 3.000000\nluis   4.358899 0.000000 3.464102 1.414214\narthur 1.732051 3.464102 0.000000 2.449490\nmurilo 3.000000 1.414214 2.449490 0.000000\n\n\nAnd perform a hierarchical clustering again:\n\nhc &lt;- hclust(as.dist(distance_matrix))\nplot(hc)\n\n\n\n\n\n\n\n\n… well, same results. The groups are well defined. Maybe with more dimensions, the groups would start to get more mixed.\nAnd that’s it. Sometimes, basic geometry and a bit of “magic” can solve problems without the need for complex approaches.\nOf course, this example is far too simple to be applied at a large scale. There are other techniques, like DBSCAN, Spectral Clustering, and Self-Organizing Maps, that are better suited for more complex problems or different types of data.\nBut it’s unquestionable that our fellow ancient mathematicians, who discovered the relation we now call the Pythagorean theorem, also planted a seed — a seed that grew to become one of the foundations of modern mathematics and its offspring, like machine learning and statistics.\nSo next time someone tells you that math is boring or useless, you can tell them that even grouping people by their favorite food can be done using triangles.\nGeometry isn’t just about shapes on paper — it’s about understanding the world in ways you never imagined.\nAnd hey, if a 2500-year-old idea can still help us make sense of today’s data, maybe math is kind of magical after all, right?"
  },
  {
    "objectID": "projects/dice_simulations/index.html",
    "href": "projects/dice_simulations/index.html",
    "title": "Simulating dice throws to understand how probabilities work",
    "section": "",
    "text": "In a lecture about probabilities, after many examples using dices and colored balls, one of my students challenged me to throw three dices. If the sum of the faces summed more than 12, I would have to dismiss the class or let them play cards instead of studying.\nI got intrigued about the odds of this bet. What is the probabilities of them winning this? Well, with some calculation I was able to found that the probabilities of them winning were about 25.9%, what seemed pretty high for me, given that the max sum of the three dices is 18. So, to confront my theoretical approach, I decided to construct a computational simulation of this.\nBasically, we just need to create something that simulates dice throws, what is pretty easy:\n\nimport random\n\nn_dices = 3 # number of dices\n\n# Here we roll the dices\ndice_throws = [random.randint(1, 6) for _ in range(n_dices)]\nprint(f\"Rolled: {dice_throws}\")\n\nRolled: [1, 5, 3]\n\nprint(f\"Sum: {sum(dice_throws)}\")\n\nSum: 9\n\n\nNice.\nHowever, 1 repetition does not make statistics, so we need to throw it many times if we want to test something. So, we will throw three dices five times, storing its sum in a list, just to test the ideia:\n\nimport random\n\nn_dices = 3 # number of dices\nrepetitions = 5 # number of repetitions\n\n# Here we roll the dices\nexperiment = []\nfor i in range(0, repetitions):\n  \n  dice_throws = [random.randint(1, 6) for _ in range(n_dices)]\n  dice_sum = sum(dice_throws)\n  experiment.append(dice_sum)\n  \nprint(experiment)\n\n[6, 9, 17, 13, 8]\n\n\nThis is the result of our experiment (for now). But to make it serious, we need to increase the number of repetitions. Let us make it 1000. Also, we are only interested in throws that summed &gt; 12, so lets only count those. Also, I will set a seed for the randomness generator, just for it to be reproducible:\n\nimport random\nrandom.seed(13) # set seed\n\nn_dices = 3 # number of dices\nrepetitions = 1000 # number of repetitions\n\n# Here we roll the dices\nexperiment = []\nfor i in range(0, repetitions):\n  \n  dice_throws = [random.randint(1, 6) for _ in range(n_dices)]\n  dice_sum = sum(dice_throws)\n  experiment.append(dice_sum)\n  \ngreater_than_12 = sum([n &gt; 12 for n in experiment])\nprint(greater_than_12)\n\n251\n\n\nWow, 251 of 1000 throws. It’s 25,1% of chance! Pretty close to the theoretical 25,9%… but not enough. Let us expand that. We will now include replicates, something very important in experiment designs. So, we will run the experiment 10 times, and compute the probabilities for each. We will consider our final result as the arithmetic mean of those.\nYou know what? Let’s make 100,000 repetitions… and remove the seed, to ensure it is random!\n\nimport random\nimport statistics as st\nrandom.seed(13) # set seed\n\nn_dices = 3 # number of dices\nrepetitions = 100000 # number of repetitions\nreplicates = 5\n\n# Here we roll the dices\nexperiment = []\nfor j in range(0, replicates):\n  rolls = [] \n  for i in range(0, repetitions):\n  \n    dice_throws = [random.randint(1, 6) for _ in range(n_dices)]\n    dice_sum = sum(dice_throws)\n    rolls.append(dice_sum)\n  \n  greater_than_12 = sum([n &gt; 12 for n in rolls])\n  prob = greater_than_12/repetitions\n    \n  experiment.append(prob)\n\nprint(st.mean(experiment))\n\n0.259412\n\n\nWow, 25.9412%. Pretty close, huh?\nProbabilities are amazing."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Fitting a Linear Model with Gradient Descent (from Scratch in R)\n\n\n\nmath\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\nAdmir Junior\n\n\nNov 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGrouping students with the help of Pythagoras\n\n\n\nmath\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\nAdmir Junior\n\n\nOct 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimulating dice throws to understand how probabilities work\n\n\n\nsimulation\n\n\nstatistics\n\n\npython\n\n\n\n\n\n\n\nAdmir Junior\n\n\nDec 11, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "This is an essay written back on college. It was my final work to graduate in Biological Sciences. Here I argument in favor of the inclusion of poetry as a way to teach and learn science."
  },
  {
    "objectID": "publications.html#becoming-eearth-2023",
    "href": "publications.html#becoming-eearth-2023",
    "title": "Publications",
    "section": "",
    "text": "This is an essay written back on college. It was my final work to graduate in Biological Sciences. Here I argument in favor of the inclusion of poetry as a way to teach and learn science."
  },
  {
    "objectID": "projects/gradient_exploration/index.html",
    "href": "projects/gradient_exploration/index.html",
    "title": "Fitting a Linear Model with Gradient Descent (from Scratch in R)",
    "section": "",
    "text": "One of the principal concepts in machine learning is the gradient descent. It is used in many important algorithms, like Gradient Boosting Machines and Neural Networks. However, people who use those algorithms often do not understand how it works. Here, I will try to introduce this amazing process with a very simple example. First, let’s simulate some data. I will generate a variable \\(x\\) that is just 50 random numbers between 0 and 100:\n\nset.seed(13)\nx &lt;- sort(sample(1:100, size = 50))\nx\n\n [1]  1  3  4  5  6  7 11 12 15 22 23 24 29 30 32 42 47 48 49 51 54 55 56 60 61\n[26] 62 63 64 65 66 68 69 70 73 74 75 77 78 81 82 83 84 85 88 89 90 96 97 98 99\n\n\nNow I will create a variable \\(y\\) that will follow a linear relationship with \\(x\\) determined by \\(y = 2x +3\\); but I will also introduce some random noise, that will prevent the data to be fitted exactly by the equation above; in other words, I will introduce what is called residuals (\\(\\varepsilon\\)), to add some spice. So the equation that I will use to create \\(y\\) is \\(y=2x+3+\\epsilon\\) (I will be back on this soon):\n\nset.seed(13)\nresiduals_noise &lt;- sample(-15:15,50,replace = T)\ny = (2*x) + 3 + residuals_noise\ny\n\n [1]  13  -4   0   7  12   7  25  33  21  60  61  59  64  64  61  86 104 106  90\n[20] 101 112 119 123 136 124 128 141 132 128 123 147 148 141 162 156 140 172 146\n[39] 154 181 157 185 159 164 195 186 203 212 202 207\n\n\nNice. Let’s plot this:\n\nplot(y~x)\n\n\n\n\n\n\n\n\nAwesome. Pretty linear to make it simple, the way teachers like. Now, remember that I said that the equation for this is \\(y = 2x +3 + \\varepsilon\\)? Well, I guess that if you is already familiar with linear models you recognized this equation. This because the observed data in a linear model is defined as \\(y = \\beta_0 + \\beta_1x + \\varepsilon\\). In this, \\(\\varepsilon\\) still means the random error of the model, and \\(\\beta_0\\) and \\(\\beta_1\\) are the parameters. In this article, we will try to find a good-fitting model for this data using the gradient and backpropagation."
  },
  {
    "objectID": "projects/gradient_exploration/index.html#introduction",
    "href": "projects/gradient_exploration/index.html#introduction",
    "title": "Fitting a Linear Model with Gradient Descent (from Scratch in R)",
    "section": "",
    "text": "One of the principal concepts in machine learning is the gradient descent. It is used in many important algorithms, like Gradient Boosting Machines and Neural Networks. However, people who use those algorithms often do not understand how it works. Here, I will try to introduce this amazing process with a very simple example. First, let’s simulate some data. I will generate a variable \\(x\\) that is just 50 random numbers between 0 and 100:\n\nset.seed(13)\nx &lt;- sort(sample(1:100, size = 50))\nx\n\n [1]  1  3  4  5  6  7 11 12 15 22 23 24 29 30 32 42 47 48 49 51 54 55 56 60 61\n[26] 62 63 64 65 66 68 69 70 73 74 75 77 78 81 82 83 84 85 88 89 90 96 97 98 99\n\n\nNow I will create a variable \\(y\\) that will follow a linear relationship with \\(x\\) determined by \\(y = 2x +3\\); but I will also introduce some random noise, that will prevent the data to be fitted exactly by the equation above; in other words, I will introduce what is called residuals (\\(\\varepsilon\\)), to add some spice. So the equation that I will use to create \\(y\\) is \\(y=2x+3+\\epsilon\\) (I will be back on this soon):\n\nset.seed(13)\nresiduals_noise &lt;- sample(-15:15,50,replace = T)\ny = (2*x) + 3 + residuals_noise\ny\n\n [1]  13  -4   0   7  12   7  25  33  21  60  61  59  64  64  61  86 104 106  90\n[20] 101 112 119 123 136 124 128 141 132 128 123 147 148 141 162 156 140 172 146\n[39] 154 181 157 185 159 164 195 186 203 212 202 207\n\n\nNice. Let’s plot this:\n\nplot(y~x)\n\n\n\n\n\n\n\n\nAwesome. Pretty linear to make it simple, the way teachers like. Now, remember that I said that the equation for this is \\(y = 2x +3 + \\varepsilon\\)? Well, I guess that if you is already familiar with linear models you recognized this equation. This because the observed data in a linear model is defined as \\(y = \\beta_0 + \\beta_1x + \\varepsilon\\). In this, \\(\\varepsilon\\) still means the random error of the model, and \\(\\beta_0\\) and \\(\\beta_1\\) are the parameters. In this article, we will try to find a good-fitting model for this data using the gradient and backpropagation."
  },
  {
    "objectID": "projects/gradient_exploration/index.html#weights-and-bias",
    "href": "projects/gradient_exploration/index.html#weights-and-bias",
    "title": "Fitting a Linear Model with Gradient Descent (from Scratch in R)",
    "section": "Weights and bias",
    "text": "Weights and bias\nNow things start getting fun. See, in classical statistics, if we define a linear model for this data, it will look like this \\[\n  \\hat{y} = \\beta_0 + \\beta_1x\n  \\]\nwhere \\(\\hat{y}\\) is the predicted \\(y\\) value. \\(\\beta_0\\) and \\(\\beta_1\\) are also known as linear and angular coefficients or intercept and slope, respectively, depending on the context. The random error \\(\\varepsilon\\) is not possible to introduce directly the linear model - it is due to real-world variance. In machine learning, \\(\\beta_1\\) is often called “weight” (\\(w\\)), because it multiplies the \\(x\\) variable, and \\(\\beta_0\\) is called “bias” \\(b\\), because it adds a constant value to the predictions. So, we could rewrite the equation as\n\\[\n\\hat{y}=wx + b\n\\]\nWell, the most obvious model we could fit for this data clearly is the equation I used to define \\(y\\), without the random error, i.e., \\(\\hat{y} = 2x + 3\\); with \\(\\beta_0 = b = 3\\) and \\(\\beta_1= w = 2\\). Let’s assume this model, generate some predictions and plot it in a line:\n\ny_hat &lt;- (2 * x)+3 # making predictions of the ideal model\n\nplot(y ~ x)\npoints(y_hat ~ x, col = \"red\", cex = 0.5) # adding predictions for x\nabline(a=3,b=2, col = \"red\") # adding the prediction line of the model\n\n\n\n\n\n\n\n\nSee? The red line is our linear model. The red dots are the predicted \\(y\\) values (\\(\\hat{y}\\)). To evaluate this, we need a metric. I’ll use the Mean Squared Error (MSE, but here we will refer it as \\(L\\), for loss, also). The MSE is defined as\n\\[\nL = MSE = {{1 \\over n} \\sum_{i=1}^n (y-\\hat{y})^2}\n\\]\nwhere \\(n\\) is the number of samples. This can be implemented in R as a function, our loss function \\(L\\), what will be useful later on:\n\nMSE &lt;- function(y,y_hat) {\n  squared_error &lt;- (y - y_hat)**2\n  return(mean(squared_error)) \n}\n\nMSE(y, y_hat)\n\n[1] 88.3\n\n\nNice. Now we know that, for a linear model with \\(w = 2; b = 3\\), the loss is \\(MSE = 88.3\\). Remember that."
  },
  {
    "objectID": "projects/gradient_exploration/index.html#yes-but-what-about-the-gradient",
    "href": "projects/gradient_exploration/index.html#yes-but-what-about-the-gradient",
    "title": "Fitting a Linear Model with Gradient Descent (from Scratch in R)",
    "section": "Yes… but what about the gradient?",
    "text": "Yes… but what about the gradient?\nIn the previous sections we were constructing our foundations to now finally construct the gradient descent. Basically, the task here is to define random initialization values for \\(w\\) and \\(b\\), make predictions, calculate the loss (\\(L\\)), compute the gradient, update the parameters, and so on, until we find the best model. Each iteration of the process is called an epoch. The gradient here is just the information on how much and in which direction should the value of a parameter be updated, e.g., if in the gradient computation step we find that the gradient equals \\(-0.35\\), this means that increasing the parameter slightly would increase the loss, so we should move it in the opposite direction — that is, we should increase the parameter by \\(0.35 \\cdot \\text{lr}\\). The \\(\\text{lr}\\) stands for learning rate and is the proportion of the gradient used, i.e., it controls how much the parameters will change between one epoch and the other.\nAs you can guess, we need to compute one gradient for each parameter to be adjusted, but let’s start simple, adjusting just the weight \\(w\\). In this case, the gradient is defined as \\(\\frac{dL}{dw}\\), i.e, the derivative of the loss \\(L\\), in our case the MSE, with respect to the weight \\(w\\). In short, how much the weight influences in the loss. If the reader is not familiarized with derivatives at all, I do recommend searching about it; in any case, just go with the flux and everything will be okay. You should have noticed by now, however, that \\(w\\) its not really a term of \\(L\\). Instead, \\(w\\) is a term of \\(\\hat{y}\\), and \\(\\hat{y}\\) is a term of \\(L\\). So, by the laws of calculus (in special the chain rule), we can say that\n\\[\n\\frac{dL}{dw} = \\frac{dL}{d\\hat{y}} \\cdot \\frac{d\\hat{y}}{dw}\n\\]\nand I think that it is beautiful. Now, by the same set of rules, the derivative of the predicted \\(y\\) with respect to the weight \\(w\\) equals \\(x\\), i.e., \\(\\frac{d\\hat{y}}{dw} =x\\). And \\(\\frac{dL}{d\\hat{y}}=-\\frac{2}{n}(y-\\hat{y})\\)."
  },
  {
    "objectID": "projects/gradient_exploration/index.html#finally-fitting-the-model",
    "href": "projects/gradient_exploration/index.html#finally-fitting-the-model",
    "title": "Fitting a Linear Model with Gradient Descent (from Scratch in R)",
    "section": "Finally, fitting the model",
    "text": "Finally, fitting the model\nEnough with derivatives, let’s do some coding work. First, let’s set a random value \\(w\\), the \\(lr = 0.00001\\), and get the first version of our model:\n\nset.seed(13)\nlr &lt;- 0.00001\nw &lt;- runif(1)\nprint(paste(\"Our initial w:\",w))\n\n[1] \"Our initial w: 0.710322446655482\"\n\n\n\ny_hat &lt;- (w * x)+3 # our new model\n\nplot(y ~ x)\npoints(y_hat ~ x, col = \"blue\", cex = 0.5) # adding predictions for x\nabline(a=3,b=w, col = \"blue\") # the model's predictions line\nabline(a=3,b=2, col = \"red\") # keep the old line as reference\n\nloss &lt;- MSE(y,y_hat)\ntext(x = 0, y=200,paste(\"Loss:\",round(loss,3)), pos=4)\n\n\n\n\n\n\n\n\nHorrendous. So wee need to compute the gradient:\n\ndl_dyhat &lt;- -2 * (y - y_hat) # derivative of L with respect to y_hat, for each point\ndyhat_dw &lt;- x                # derivative of y_hat with respect to w, for each point\n\ngradient &lt;- mean(dl_dyhat * dyhat_dw) \ngradient\n\n[1] -10131.17\n\n\nand update the weight:\n\nw &lt;- w - (lr * gradient)\nw\n\n[1] 0.8116342\n\n\nThis is the core of gradient descent: we move \\(w\\) in the opposite direction of the gradient.\nLet’s update the model:\n\ny_hat &lt;- (w * x)+3 # our new model\n\nplot(y ~ x)\npoints(y_hat ~ x, col = \"blue\", cex = 0.5) # adding predictions for x\nabline(a=3,b=w, col = \"blue\") # the model's predictions line\nabline(a=3,b=2, col = \"red\") # keep the old line as reference\n\nloss &lt;- MSE(y,y_hat)\ntext(x = 0, y=200,paste(\"Loss:\",round(loss,3)), pos=4)\n\n\n\n\n\n\n\n\nOkay… loss reduced. Let’s do it all again:\n\ny_hat &lt;- (w * x) + 3\n\ndl_dyhat &lt;- -2 * (y - y_hat)\ndyhat_dw &lt;- x\ngradient &lt;- mean(dl_dyhat * dyhat_dw)\n\nw &lt;- w - lr * gradient\n\ny_hat &lt;- (w * x) + 3\nloss &lt;- MSE(y, y_hat)\n\nplot(y ~ x)\npoints(y_hat ~ x, col = \"blue\", cex = 0.5)\nabline(a = 3, b = w, col = \"blue\")     # Our model\nabline(a = 3, b = 2, col = \"red\")      # Ideal model\ntext(x = 0, y = max(y), paste(\"Loss:\", round(loss, 3)), pos = 4)\n\n\n\n\n\n\n\n\nNice! We can see that the loss keeps going down — our model is improving!\nAs I said, we can do this many times, i.e., many epochs, and search for the best model. Here we will set 100 epochs, but let’s go over the first 10 to see some things:\n\nepochs &lt;- 1:100\n\nlosses &lt;- c() # this will be useful later\nfor (epoch in epochs[1:10]) {\n  y_hat &lt;- (w * x) + 3\n\n  dl_dyhat &lt;- -2 * (y - y_hat)\n  dyhat_dw &lt;- x\n  gradient &lt;- mean(dl_dyhat * dyhat_dw)\n\n  w &lt;- w - lr * gradient\n\n  loss &lt;- MSE(y, y_hat)\n  losses &lt;- c(losses, loss)\n  \n  print(paste(\"Epoch:\",epoch,\"| Loss:\",loss,\"| w:\",w))\n}\n\n[1] \"Epoch: 1 | Loss: 4935.07135017847 | w: 0.991498528705445\"\n[1] \"Epoch: 2 | Loss: 4218.28184732541 | w: 1.07120186677443\"\n[1] \"Epoch: 3 | Loss: 3607.42958780821 | w: 1.14478000331282\"\n[1] \"Epoch: 4 | Loss: 3086.85767447514 | w: 1.21270366005823\"\n[1] \"Epoch: 5 | Loss: 2643.22320684554 | w: 1.27540738378275\"\n[1] \"Epoch: 6 | Loss: 2265.15528610646 | w: 1.33329232633905\"\n[1] \"Epoch: 7 | Loss: 1942.96356494536 | w: 1.38672881105989\"\n[1] \"Epoch: 8 | Loss: 1668.3898719914 | w: 1.43605870192994\"\n[1] \"Epoch: 9 | Loss: 1434.39654469334 | w: 1.48159759068662\"\n[1] \"Epoch: 10 | Loss: 1234.98604534445 | w: 1.52363681584236\"\n\n\nNow its possible to see how the loss reduces every epoch as \\(w\\) get closes to 2, our ideal value. However, we need to go further, so let’s go for 90 more epochs.\n\nepochs &lt;- 1:100\n\nfor (epoch in epochs[11:100]) {\n  y_hat &lt;- (w * x) + 3\n\n  dl_dyhat &lt;- -2 * (y - y_hat)\n  dyhat_dw &lt;- x\n  gradient &lt;- mean(dl_dyhat * dyhat_dw)\n\n  w &lt;- w - lr * gradient\n\n  loss &lt;- MSE(y, y_hat)\n  losses &lt;- c(losses, loss)\n}\n\nWe can visualize the training process with a graph of loss against each epoch:\n\nplot(losses ~ epochs, type=\"l\", ylab = \"Loss (MSE)\", xlab = \"Epoch\")\n\n\n\n\n\n\n\n\nThis type of plot is called “convergence plot”. It indicates that around epoch 40 the model converged, i.e., minimized the loss, and could not improve further. That is, we found the model we were seeking. Awesome, isn’t it? This is the essence of many machine learning algorithms: define a model, measure the error, and use gradients to make it better — one small step at a time.\nWell, let’s visualize this model:\n\nplot(y ~ x)\npoints(y_hat ~ x, col = \"blue\", cex = 0.5)\nabline(a = 3, b = w, col = \"blue\")     # Our model\nabline(a = 3, b = 2, col = \"red\")      # Ideal model\ntext(x = 0, y = max(y), paste(\"Loss:\", round(loss, 3)), pos = 4)\ntext(x = 0, y = max(y)-15, paste(\"w:\", round(w, 3)), pos = 4)\n\n\n\n\n\n\n\n\nVery nice. We ended up with a model \\(\\hat{y}=2.028x +3\\). Very close of our first guessed model \\(\\hat{y}=2x+3\\); and look, the loss is lower!! It was 88.3, now it is 85.152. It means we constructed a model more accurate."
  },
  {
    "objectID": "projects/gradient_exploration/index.html#getting-more-complex",
    "href": "projects/gradient_exploration/index.html#getting-more-complex",
    "title": "Fitting a Linear Model with Gradient Descent (from Scratch in R)",
    "section": "Getting more complex",
    "text": "Getting more complex\nWow, we got so far, but let’s go even further. Now, we will also apply the gradient to find the bias \\(b\\). For this, we will introduce the gradient to adjust \\(b\\), i.e., the derivative of \\(L\\) with respect to \\(b\\), or \\(\\frac{dL}{db}\\), which can be resolved in the same way as the gradient of \\(w\\), with the chain rule. This time I’ll skip the math, let’s go straight to the code:\n\nset.seed(13)\n\n# Parameter initialization\nw &lt;- runif(1)\nb &lt;- runif(1)\nlr &lt;- 0.00001\n\n# Para guardar o histórico da perda\nlosses &lt;- c()\nepochs &lt;- 1:100\n\n# Training loop\nfor (epoch in epochs) {\n  # Predictions\n  y_hat &lt;- (w * x) + b\n\n  # Gradients\n  dl_dyhat &lt;- -2 * (y - y_hat)    # ∂L/∂ŷ\n  dyhat_dw &lt;- x                   # ∂ŷ/∂w\n  dyhat_db &lt;- 1                   # ∂ŷ/∂b\n\n  gradient_w &lt;- mean(dl_dyhat * dyhat_dw)\n  gradient_b &lt;- mean(dl_dyhat * dyhat_db)\n\n  # Parameters update\n  w &lt;- w - lr * gradient_w\n  b &lt;- b - lr * gradient_b\n\n  # Compute the loss\n  loss &lt;- MSE(y, y_hat)\n  losses &lt;- c(losses, loss)\n}\n\nLet’s see the convergence:\n\nplot(losses ~ epochs, type=\"l\", ylab = \"Loss (MSE)\", xlab = \"Epoch\")\n\n\n\n\n\n\n\n\nAnd visualize the final model:\n\nplot(y ~ x)\npoints(y_hat ~ x, col = \"blue\", cex = 0.5)\nabline(a = b, b = w, col = \"blue\")     # Final model\nabline(a = 3, b = 2, col = \"red\")      # First model\ntext(x = 0, y = max(y), paste(\"Loss:\", round(loss, 3)), pos = 4)\ntext(x = 0, y = max(y)-15, paste(\"w:\", round(w, 3)), pos = 4)\ntext(x = 0, y = max(y)-30, paste(\"bias:\", round(b, 3)), pos = 4)\n\n\n\n\n\n\n\n\nThus, the equation of our final model is \\(\\hat{y_i}=wx_i+b=2.067x+0.266\\). Note that the estimated \\(b\\) was pretty distant from 3, our firstly estimated \\(b\\). However, we managed to reduced the loss even more, now it is 84.566. So, basically, we ended up constructing a better model than the first one!! Pretty neat, huh?\n# The grand finale\nWe are pretty much done here, but I want to construct an animation of the full process just for better visualization… and because it is truly beautiful:\n\nlibrary(ggplot2)\nlibrary(gganimate)\n\nset.seed(13)\n# Initializing parameters\nw &lt;- runif(1)\nb &lt;- runif(1)\nlr &lt;- 0.00001\n\n# Store history\nhistory &lt;- data.frame()\n\n# Training loop\nepochs &lt;- 1:100\nfor (epoch in epochs) {\n  y_hat &lt;- w * x + b\n  \n  # Gradient\n  dl_dyhat &lt;- -2 * (y - y_hat)\n  gradient_w &lt;- mean(dl_dyhat * x)\n  gradient_b &lt;- mean(dl_dyhat)\n  \n  # Parameter update\n  w &lt;- w - lr * gradient_w\n  b &lt;- b - lr * gradient_b\n  \n  # Loss\n  loss &lt;- mean((y - y_hat)^2)\n  \n  # Register it all\n  step_df &lt;- data.frame(\n    x = x,\n    y = y,\n    y_hat = y_hat,\n    epoch = epoch,\n    w = w,\n    b = b,\n    loss = loss\n  )\n  \n  history &lt;- rbind(history, step_df)\n}\n\nmax_y &lt;- max(y)\n\n# Animated graph\np &lt;- ggplot(history, aes(x = x, y = y)) +\n  geom_point(color = \"black\", alpha = 0.6) +\n  geom_line(aes(y = y_hat), color = \"blue\", size = 1) +\n  geom_line(aes(y = (2*x)+3), color = \"red\", size = 1) +\n  geom_text(aes(\n    x = 10,\n    y = max_y,\n    label = paste0(\"w = \", round(w, 4),\n                   \", b = \", round(b, 4),\n                   \", loss = \", round(loss, 1))\n  ), hjust = 0.35, vjust = 1, size = 5, color = \"black\") +\n  labs(\n    title = 'Epoch: {closest_state}',\n    x = 'x', y = 'y'\n  ) +\n  theme_minimal() +\n  transition_states(epoch, transition_length = 1, state_length = 1, wrap = FALSE) +\n  ease_aes('linear')\n\n# Render\n# anim &lt;- animate(p, fps = 10, nframes = length(epochs), width = 800, height = 400,\n#                 renderer = gifski_renderer(\"animated_gradient.gif\"))\n\n\nThat’s all!!\nThank you."
  }
]